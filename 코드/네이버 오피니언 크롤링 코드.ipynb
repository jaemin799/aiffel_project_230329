{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d62b92a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤러를 만들기 전 필요한 도구들을 임포트합니다.\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 페이지 수, 카테고리, 날짜를 입력값으로 받습니다.\n",
    "def make_urllist(page_num, code, date): \n",
    "  urllist= []\n",
    "  for i in range(1, page_num + 1):\n",
    "    url = 'https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1='+str(code)+'&date='+str(date)+'&page='+str(i)\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.90 Safari/537.36'}\n",
    "    news = requests.get(url, headers=headers)\n",
    "\n",
    "    # BeautifulSoup의 인스턴스 생성합니다. 파서는 html.parser를 사용합니다.\n",
    "    soup = BeautifulSoup(news.content, 'html.parser')\n",
    "\n",
    "    # CASE 1\n",
    "    news_list = soup.select('.newsflash_body .type06_headline li dl')\n",
    "    # CASE 2\n",
    "    news_list.extend(soup.select('.newsflash_body .type06 li dl'))\n",
    "        \n",
    "    # 각 뉴스로부터 a 태그인 <a href ='주소'> 에서 '주소'만을 가져옵니다.\n",
    "    for line in news_list:\n",
    "        urllist.append(line.a.get('href'))\n",
    "  return urllist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30ae7f5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "뉴스 기사의 개수:  20\n"
     ]
    }
   ],
   "source": [
    "url_list = make_urllist(1, 110, 20230330)\n",
    "print('뉴스 기사의 개수: ',len(url_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fad3d243",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {'110' : '오피니언'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c005322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "\n",
    "#- 데이터프레임을 생성하는 함수입니다.\n",
    "def make_data(urllist, code):\n",
    "  text_list = []\n",
    "  for url in urllist:\n",
    "    article = Article(url, language='ko')\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    text_list.append(article.text)\n",
    "\n",
    "  #- 데이터프레임의 'news' 키 아래 파싱한 텍스트를 밸류로 붙여줍니다.\n",
    "  df = pd.DataFrame({'news': text_list})\n",
    "\n",
    "  #- 데이터프레임의 'code' 키 아래 한글 카테고리명을 붙여줍니다.\n",
    "  df['code'] = idx2word[str(code)]\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69747209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[110]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_list = [110]\n",
    "\n",
    "code_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe06ef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import random\n",
    "import time, os\n",
    "\n",
    "def make_total_data(page_num, code_list, date):\n",
    "  start = int(time.time())  \n",
    "  num_cores = 4  \n",
    "  df = None\n",
    "  for code in code_list:\n",
    "    pool = Pool(num_cores)\n",
    "    url_list = make_urllist(page_num, code, date)\n",
    "    df_temp = make_data(url_list, code)\n",
    "    print(str(code)+'번 코드에 대한 데이터를 만들었습니다.')\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    time.sleep(random.randint(0,1))\n",
    "    if df is not None:\n",
    "      df = pd.concat([df, df_temp])\n",
    "    else:\n",
    "      df = df_temp\n",
    "\n",
    "  print(\"***run time(sec) :\", int(time.time()) - start)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ae6c6be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110번 코드에 대한 데이터를 만들었습니다.\n",
      "***run time(sec) : 11\n"
     ]
    }
   ],
   "source": [
    "#데이터셋 확보를 위해 쓴 코드는 df = make_total_data(10, code_list, 20230317~20230328까지 총 12파일 사용)\n",
    "\n",
    "\n",
    "df = make_total_data(1, code_list, 20230330)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82ed05c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aiffel/aiffel/news_crawler/news_data.csv File Saved!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 데이터프레임 파일을 csv 파일로 저장합니다.\n",
    "# 저장경로는 이번 프로젝트를 위해 만든 폴더로 지정해 주세요.\n",
    "csv_path = os.getenv(\"HOME\") + \"/aiffel/news_crawler/news_data.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "if os.path.exists(csv_path):\n",
    "  print('{} File Saved!'.format(csv_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0def94d9",
   "metadata": {},
   "source": [
    "### 출처 https://velog.io/@w00sah0101/FUNDAMENTALS-31.-%EB%89%B4%EC%8A%A4%EA%B8%B0%EC%82%AC-%ED%81%AC%EB%A1%A4%EB%A7%81-%EB%B0%8F-%EB%B6%84%EB%A5%98#8-%EB%84%A4%EC%9D%B4%EB%B2%84-%EB%89%B4%EC%8A%A4-%EA%B8%B0%EC%82%AC-%ED%81%AC%EB%A1%A4%EB%A7%81-3-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%88%98%EC%A7%91-%EB%B0%8F-%EC%A0%84%EC%B2%98%EB%A6%AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a69dfe9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
